{"version":1,"ops":[{"type":6,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603002326,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlRWRpdDo0NDMyNDc5MjU="},"target":"148f84d0c8729128ae58e9965a47a89c0f73405dfd88e31aca400e0115500bb7","message":"Wang Yi claims that all tests are systematically wrong if run with the same seed: \nhttps://github.com/wangyi-fudan/wyhash/issues/75\n\ndue to the https://en.wikipedia.org/wiki/Multiple_comparisons_problem\nbecause when you make 1000 statistical tests, some of them will show a p-value like 0.001, it is natural.\nyou can run your program with different seed. \"weakness\" due to random chance will disappear with different seed while systematic fail will persist.","files":null},{"type":6,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603004175,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlRWRpdDo0NDMyNTA4MTk="},"target":"148f84d0c8729128ae58e9965a47a89c0f73405dfd88e31aca400e0115500bb7","message":"Wang Yi claims that all tests are systematically wrong : \nhttps://github.com/wangyi-fudan/wyhash/issues/75\ndue to the https://en.wikipedia.org/wiki/Multiple_comparisons_problem\nbecause when you make 1000 statistical tests, some of them will show a p-value like 0.001, it is natural.\nyou can run your program with different seed. \"weakness\" due to random chance will disappear with different seed while systematic fail will persist.","files":null},{"type":5,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603002343,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDM4ODk5NDY2NDA="},"added":["bug"],"removed":[]},{"type":2,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603004149,"metadata":{"github-id":"MDE3OlJlbmFtZWRUaXRsZUV2ZW50Mzg4OTk2MzY3OQ=="},"title":"Investigate test weaknesses: multiple testing problem","was":"Investigate test weaknesses: multiple testing problem"},{"type":3,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603004275,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcxMTEyNjIxNQ==","github-url":"https://github.com/rurban/dieharder/issues/6#issuecomment-711126215"},"message":"In this case its not a Marsaglia (diehard) or STS test, but a new rgb_lagged_sum test with ntup=17.","files":null},{"type":3,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603005965,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcxMTEyODUzNQ==","github-url":"https://github.com/rurban/dieharder/issues/6#issuecomment-711128535"},"message":"Ok, so the recommended procedure if you get unexpected weak results is to use -Y 1. \n\nXtrategy \"resolve ambiguity\"\n\n1 - 'resolve ambiguity' (RA) mode.  If a test returns 'weak', this is \n     an undesired result.  What does that mean, after all?  If you run a long\n     test series, you will see occasional weak returns for a perfect\n     generators because p is uniformly distributed and will appear in any\n     finite interval from time to time.  Even if a test run returns more than\n     one weak result, you cannot be certain that the generator is failing.\n     RA mode adds psamples (usually in blocks of 100) until the\n     test result ends up solidly not weak or proceeds to unambiguous failure.\n     This is morally equivalent to running the test several times to see if a\n     weak result is reproducible, but eliminates the bias of personal\n     judgement in the process since the default failure threshold is very\n     small and very unlikely to be reached by random chance even in many\n     runs.","files":null},{"type":6,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603005965,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA3NDQwNTQw"},"target":"76f95605d5d2849c5f3b3f2072e82280aa9abcab247e2559da01466a8425f2af","message":"Ok, so the recommended procedure if you get unexpected weak results is to use -Y 1. \n\nXtrategy \"resolve ambiguity\"\n\n1 - 'resolve ambiguity' (RA) mode.  If a test returns 'weak', this is \n     an undesired result.  What does that mean, after all?  If you run a long\n     test series, you will see occasional weak returns for a perfect\n     generators because p is uniformly distributed and will appear in any\n     finite interval from time to time.  Even if a test run returns more than\n     one weak result, you cannot be certain that the generator is failing.\n     RA mode adds psamples (usually in blocks of 100) until the\n     test result ends up solidly not weak or proceeds to unambiguous failure.\n     This is morally equivalent to running the test several times to see if a\n     weak result is reproducible, but eliminates the bias of personal\n     judgement in the process since the default failure threshold is very\n     small and very unlikely to be reached by random chance even in many\n     runs.\n\nDo this for all GOOD and WEAK results. See QUALITY.md","files":null},{"type":3,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603019133,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcxMTE1MTYyMQ==","github-url":"https://github.com/rurban/dieharder/issues/6#issuecomment-711151621"},"message":"I either\n\n* have to eliminate expected outliers as in smhasher (needs lot of time and memory, which is harder with that many prng results).\n* Or substract the alpha (family-wise error rate). (with this I loose some raw data, but should be done).\n* Or I use a different testing strategy (-Y1 in dieharder), which checks in weak results again with different seeds to resolve potential ambiguities in bad p-values. (this is already builtin)","files":null},{"type":5,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603106378,"metadata":{"github-id":"MDE0OlVubGFiZWxlZEV2ZW50Mzg5MjgzMzIyMQ=="},"added":[],"removed":["bug"]},{"type":5,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603106378,"metadata":{"github-id":"MDEyOkxhYmVsZWRFdmVudDM4OTI4MzMyMjI="},"added":["enhancement"],"removed":[]},{"type":3,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603538604,"metadata":{"github-id":"MDEyOklzc3VlQ29tbWVudDcxNTkwMDkzOQ==","github-url":"https://github.com/rurban/dieharder/issues/6#issuecomment-715900939"},"message":"-Y1 is recommended by TestU01:\n\n`When a p-value is extremely close to 0 or to 1 (for example, if it is less than 10^−10), one can obviously conclude that the generator fails the test. If the p-value is suspicious but failure is not clear enough, (p = 0.0005, for example), then the test can be replicated independently until either failure becomes obvious or suspicion disappears (i.e., one finds that the suspect p-value was obtained only by chance). This approach is possible because there is no limit (other than CPU time) on the amount of data that can be produced by a RNG to increase the sample size and the power of the test. (TestU01 manual)`","files":null},{"type":6,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603538604,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA5MDM1MDU0"},"target":"ba6e707abd1b71d20723e5f8ba1ecc42e68984d5a08e5be4f66175ce2fe7cc88","message":"Our `-Y1` strategy is recommended by TestU01. that what we use, together with `-k2`.\n\n_When a p-value is extremely close to 0 or to 1 (for example, if it is less than 10^−10), one can obviously conclude that the generator fails the test. If the p-value is suspicious but failure is not clear enough, (p = 0.0005, for example), then the test can be replicated independently until either failure becomes obvious or suspicion disappears (i.e., one finds that the suspect p-value was obtained only by chance). This approach is possible because there is no limit (other than CPU time) on the amount of data that can be produced by a RNG to increase the sample size and the power of the test. (TestU01 manual)_","files":null},{"type":6,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603538625,"metadata":{"github-id":"MDE1OlVzZXJDb250ZW50RWRpdElzc3VlQ29tbWVudEVkaXQ6NDA5MDM1MDY0"},"target":"ba6e707abd1b71d20723e5f8ba1ecc42e68984d5a08e5be4f66175ce2fe7cc88","message":"Our `-Y1` strategy is recommended by TestU01. That's what we use, together with `-k2`.\n\n_When a p-value is extremely close to 0 or to 1 (for example, if it is less than 10^−10), one can obviously conclude that the generator fails the test. If the p-value is suspicious but failure is not clear enough, (p = 0.0005, for example), then the test can be replicated independently until either failure becomes obvious or suspicion disappears (i.e., one finds that the suspect p-value was obtained only by chance). This approach is possible because there is no limit (other than CPU time) on the amount of data that can be produced by a RNG to increase the sample size and the power of the test. (TestU01 manual)_","files":null},{"type":4,"author":{"id":"f3a70076b7be1c2a8539c83ab46e8669b027a3a1"},"timestamp":1603538566,"metadata":{"github-id":"MDExOkNsb3NlZEV2ZW50MzkxNjY1ODg3OA=="},"status":2}]}